# ═══════════════════════════════════════════════
# Enterprise AI Assistant — Environment Variables
# Скопируй этот файл в .env и заполни значения
# cp .env.example .env
# ═══════════════════════════════════════════════

# ──────────────────────────────────────────────
# NEO4J — Граф знаний
# ──────────────────────────────────────────────
NEO4J_USER=neo4j
NEO4J_PASSWORD=change_me_strong_password_1

# ──────────────────────────────────────────────
# POSTGRESQL — Склад и справочники
# ──────────────────────────────────────────────
POSTGRES_DB=enterprise_ai
POSTGRES_USER=enterprise
POSTGRES_PASSWORD=change_me_strong_password_2

# ──────────────────────────────────────────────
# TELEGRAM — Бот для OpenClaw
# Получи токен у @BotFather в Telegram
# ──────────────────────────────────────────────
TELEGRAM_BOT_TOKEN=123456789:ABCDEFGHIJKLMNOP_your_token_here

# ──────────────────────────────────────────────
# OLLAMA — путь к данным моделей на хосте
# По умолчанию /home/ollama-models — после удаления проекта модели не перекачиваются.
# В Web UI: Настройки → «Путь к данным моделей Ollama». Для применения задайте здесь и перезапустите контейнеры.
# ──────────────────────────────────────────────
OLLAMA_MODELS_PATH=/home/ollama-models

# ──────────────────────────────────────────────
# AI MODELS — Имена моделей в Ollama
# Должны совпадать с именами при `ollama pull`
# LLM_MODEL используется и API, и OpenClaw (Telegram-агент)
# ──────────────────────────────────────────────
LLM_MODEL=qwen3:30b
VLM_MODEL=qwen3-vl:14b
EMBEDDING_MODEL=qwen3-embedding
RERANKER_MODEL=qwen3-reranker

# ──────────────────────────────────────────────
# EMBEDDING DIMENSION
# Размерность вектора qwen3-embedding.
# Уточни после `ollama pull qwen3-embedding` командой:
#   docker exec ollama-cpu ollama show qwen3-embedding --verbose
# Типичные значения: 1024, 2048, 4096
# ──────────────────────────────────────────────
EMBEDDING_DIM=4096

# ──────────────────────────────────────────────
# QDRANT
# ──────────────────────────────────────────────
QDRANT_COLLECTION=documents

# ──────────────────────────────────────────────
# OPENCLAW
# true  — при каждом рестарте контейнера будет
#         выполнен npm install -g openclaw@latest
# false — использовать версию из Docker-образа
# ──────────────────────────────────────────────
OPENCLAW_AUTO_UPDATE=false

# Веб-поиск в чате (опционально): Serper API key с https://serper.dev
# WEB_SEARCH_API_KEY=your_serper_api_key

# ──────────────────────────────────────────────
# WEB UI — ДОМЕН, HTTPS, АУТЕНТИФИКАЦИЯ
# ──────────────────────────────────────────────

# SERVER_HOST: публичный домен или IP сервера
#   ai.example.com  → Caddy автоматически получает Let's Encrypt сертификат
#   192.168.1.100   → Caddy генерирует self-signed сертификат (предупреждение в браузере)
SERVER_HOST=ai.example.com

# ACME_EMAIL: email для уведомлений Let's Encrypt (только при доменном режиме)
ACME_EMAIL=admin@example.com

# CADDY_DATA_PATH: каталог на хосте для хранения сертификатов Caddy (Let's Encrypt / self-signed).
# Не удаляется при make teardown или install.sh --teardown — сертификат сохраняется между переустановками.
CADDY_DATA_PATH=/var/lib/caddy-certificates

# CADDY_TLS: при лимите Let's Encrypt (429) задайте internal — Caddy будет использовать
# самоподписанный сертификат, сайт откроется (браузер покажет предупреждение).
# После истечения лимита уберите переменную и перезапустите Caddy.
# CADDY_TLS=internal

# JWT_SECRET_KEY: длинная случайная строка (мин. 32 символа)
# Генерация: openssl rand -hex 32
JWT_SECRET_KEY=change-me-generate-with-openssl-rand-hex-32
JWT_EXPIRE_HOURS=24

# CORS: разрешённые origins для React-фронтенда.
# В production = домен сервера:
# CORS_ORIGINS=https://ai.example.com
# В dev (разрешить всё):
CORS_ORIGINS=*
